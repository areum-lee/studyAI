{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 102.33964538574219)\n",
      "(1, 45.87139129638672)\n",
      "(2, 20.72882843017578)\n",
      "(3, 9.531645774841309)\n",
      "(4, 4.542611598968506)\n",
      "(5, 2.3173325061798096)\n",
      "(6, 1.3224618434906006)\n",
      "(7, 0.8753930926322937)\n",
      "(8, 0.6722511649131775)\n",
      "(9, 0.5777567625045776)\n",
      "(10, 0.5316882133483887)\n",
      "(11, 0.5072352290153503)\n",
      "(12, 0.4924611449241638)\n",
      "(13, 0.4820519983768463)\n",
      "(14, 0.4736410975456238)\n",
      "(15, 0.4661736488342285)\n",
      "(16, 0.45918023586273193)\n",
      "(17, 0.45245006680488586)\n",
      "(18, 0.44588950276374817)\n",
      "(19, 0.43945568799972534)\n",
      "(20, 0.4331285357475281)\n",
      "(21, 0.42689836025238037)\n",
      "(22, 0.4207608699798584)\n",
      "(23, 0.41471290588378906)\n",
      "(24, 0.40875244140625)\n",
      "(25, 0.4028777480125427)\n",
      "(26, 0.3970876932144165)\n",
      "(27, 0.3913811147212982)\n",
      "(28, 0.38575613498687744)\n",
      "(29, 0.38021206855773926)\n",
      "(30, 0.37474802136421204)\n",
      "(31, 0.369362473487854)\n",
      "(32, 0.3640538454055786)\n",
      "(33, 0.3588217496871948)\n",
      "(34, 0.35366499423980713)\n",
      "(35, 0.34858211874961853)\n",
      "(36, 0.3435726761817932)\n",
      "(37, 0.33863478899002075)\n",
      "(38, 0.3337680399417877)\n",
      "(39, 0.32897162437438965)\n",
      "(40, 0.32424378395080566)\n",
      "(41, 0.3195835053920746)\n",
      "(42, 0.31499096751213074)\n",
      "(43, 0.3104640245437622)\n",
      "(44, 0.30600184202194214)\n",
      "(45, 0.30160412192344666)\n",
      "(46, 0.2972695231437683)\n",
      "(47, 0.29299765825271606)\n",
      "(48, 0.28878653049468994)\n",
      "(49, 0.2846361994743347)\n",
      "(50, 0.2805456519126892)\n",
      "(51, 0.2765139043331146)\n",
      "(52, 0.27254006266593933)\n",
      "(53, 0.2686229944229126)\n",
      "(54, 0.2647625207901001)\n",
      "(55, 0.2609574496746063)\n",
      "(56, 0.2572069764137268)\n",
      "(57, 0.253510445356369)\n",
      "(58, 0.24986737966537476)\n",
      "(59, 0.24627630412578583)\n",
      "(60, 0.24273686110973358)\n",
      "(61, 0.2392483651638031)\n",
      "(62, 0.23580986261367798)\n",
      "(63, 0.2324208915233612)\n",
      "(64, 0.22908081114292145)\n",
      "(65, 0.2257886677980423)\n",
      "(66, 0.2225436568260193)\n",
      "(67, 0.21934518218040466)\n",
      "(68, 0.21619285643100739)\n",
      "(69, 0.21308578550815582)\n",
      "(70, 0.21002355217933655)\n",
      "(71, 0.20700520277023315)\n",
      "(72, 0.20403027534484863)\n",
      "(73, 0.2010979950428009)\n",
      "(74, 0.19820795953273773)\n",
      "(75, 0.19535943865776062)\n",
      "(76, 0.19255170226097107)\n",
      "(77, 0.18978436291217804)\n",
      "(78, 0.1870569884777069)\n",
      "(79, 0.18436850607395172)\n",
      "(80, 0.18171897530555725)\n",
      "(81, 0.1791074126958847)\n",
      "(82, 0.17653332650661469)\n",
      "(83, 0.17399638891220093)\n",
      "(84, 0.17149558663368225)\n",
      "(85, 0.16903099417686462)\n",
      "(86, 0.16660170257091522)\n",
      "(87, 0.16420745849609375)\n",
      "(88, 0.1618473380804062)\n",
      "(89, 0.15952152013778687)\n",
      "(90, 0.15722905099391937)\n",
      "(91, 0.15496937930583954)\n",
      "(92, 0.15274226665496826)\n",
      "(93, 0.1505468487739563)\n",
      "(94, 0.1483834683895111)\n",
      "(95, 0.1462508589029312)\n",
      "(96, 0.14414899051189423)\n",
      "(97, 0.1420772671699524)\n",
      "(98, 0.14003553986549377)\n",
      "(99, 0.1380229890346527)\n",
      "(100, 0.13603940606117249)\n",
      "(101, 0.13408437371253967)\n",
      "(102, 0.13215717673301697)\n",
      "(103, 0.13025803864002228)\n",
      "(104, 0.12838594615459442)\n",
      "(105, 0.12654095888137817)\n",
      "(106, 0.12472213804721832)\n",
      "(107, 0.12292996048927307)\n",
      "(108, 0.12116323411464691)\n",
      "(109, 0.11942195147275925)\n",
      "(110, 0.11770544946193695)\n",
      "(111, 0.11601383239030838)\n",
      "(112, 0.11434664577245712)\n",
      "(113, 0.1127033531665802)\n",
      "(114, 0.11108354479074478)\n",
      "(115, 0.10948705673217773)\n",
      "(116, 0.10791362076997757)\n",
      "(117, 0.10636269301176071)\n",
      "(118, 0.10483399033546448)\n",
      "(119, 0.10332753509283066)\n",
      "(120, 0.10184241831302643)\n",
      "(121, 0.1003788411617279)\n",
      "(122, 0.09893631190061569)\n",
      "(123, 0.09751439839601517)\n",
      "(124, 0.0961129441857338)\n",
      "(125, 0.09473173320293427)\n",
      "(126, 0.09337016940116882)\n",
      "(127, 0.09202826768159866)\n",
      "(128, 0.09070573002099991)\n",
      "(129, 0.0894022062420845)\n",
      "(130, 0.08811720460653305)\n",
      "(131, 0.08685094118118286)\n",
      "(132, 0.0856027901172638)\n",
      "(133, 0.08437246084213257)\n",
      "(134, 0.08315982669591904)\n",
      "(135, 0.08196468651294708)\n",
      "(136, 0.080786794424057)\n",
      "(137, 0.07962565869092941)\n",
      "(138, 0.07848133146762848)\n",
      "(139, 0.07735341042280197)\n",
      "(140, 0.07624182850122452)\n",
      "(141, 0.07514612376689911)\n",
      "(142, 0.07406613975763321)\n",
      "(143, 0.07300173491239548)\n",
      "(144, 0.07195247709751129)\n",
      "(145, 0.07091847062110901)\n",
      "(146, 0.06989917159080505)\n",
      "(147, 0.06889466196298599)\n",
      "(148, 0.06790456920862198)\n",
      "(149, 0.06692861020565033)\n",
      "(150, 0.06596672534942627)\n",
      "(151, 0.0650186836719513)\n",
      "(152, 0.06408434361219406)\n",
      "(153, 0.0631633996963501)\n",
      "(154, 0.06225564330816269)\n",
      "(155, 0.061360809952020645)\n",
      "(156, 0.060479044914245605)\n",
      "(157, 0.059609852731227875)\n",
      "(158, 0.05875314027070999)\n",
      "(159, 0.057908784598112106)\n",
      "(160, 0.057076580822467804)\n",
      "(161, 0.05625620484352112)\n",
      "(162, 0.05544775351881981)\n",
      "(163, 0.05465087667107582)\n",
      "(164, 0.05386548861861229)\n",
      "(165, 0.053091369569301605)\n",
      "(166, 0.05232827365398407)\n",
      "(167, 0.051576294004917145)\n",
      "(168, 0.05083506926894188)\n",
      "(169, 0.050104543566703796)\n",
      "(170, 0.049384329468011856)\n",
      "(171, 0.04867459088563919)\n",
      "(172, 0.04797515273094177)\n",
      "(173, 0.04728567227721214)\n",
      "(174, 0.046606145799160004)\n",
      "(175, 0.04593629762530327)\n",
      "(176, 0.04527609795331955)\n",
      "(177, 0.044625479727983475)\n",
      "(178, 0.04398398473858833)\n",
      "(179, 0.04335186257958412)\n",
      "(180, 0.04272890463471413)\n",
      "(181, 0.04211481660604477)\n",
      "(182, 0.04150949791073799)\n",
      "(183, 0.04091307520866394)\n",
      "(184, 0.04032503440976143)\n",
      "(185, 0.039745524525642395)\n",
      "(186, 0.039174217730760574)\n",
      "(187, 0.03861135244369507)\n",
      "(188, 0.03805633634328842)\n",
      "(189, 0.03750941902399063)\n",
      "(190, 0.036970436573028564)\n",
      "(191, 0.036439038813114166)\n",
      "(192, 0.03591533377766609)\n",
      "(193, 0.0353991836309433)\n",
      "(194, 0.03489048406481743)\n",
      "(195, 0.03438909724354744)\n",
      "(196, 0.033894866704940796)\n",
      "(197, 0.033407628536224365)\n",
      "(198, 0.032927561551332474)\n",
      "(199, 0.032454393804073334)\n",
      "(200, 0.03198792785406113)\n",
      "(201, 0.03152821213006973)\n",
      "(202, 0.0310751274228096)\n",
      "(203, 0.03062853403389454)\n",
      "(204, 0.03018832392990589)\n",
      "(205, 0.029754430055618286)\n",
      "(206, 0.029326897114515305)\n",
      "(207, 0.02890535071492195)\n",
      "(208, 0.02848992869257927)\n",
      "(209, 0.028080489486455917)\n",
      "(210, 0.027676986530423164)\n",
      "(211, 0.02727920189499855)\n",
      "(212, 0.026887202635407448)\n",
      "(213, 0.026500750333070755)\n",
      "(214, 0.026119891554117203)\n",
      "(215, 0.02574450522661209)\n",
      "(216, 0.025374460965394974)\n",
      "(217, 0.02500985935330391)\n",
      "(218, 0.02465040050446987)\n",
      "(219, 0.024296198040246964)\n",
      "(220, 0.02394697815179825)\n",
      "(221, 0.023602813482284546)\n",
      "(222, 0.02326362580060959)\n",
      "(223, 0.022929295897483826)\n",
      "(224, 0.022599734365940094)\n",
      "(225, 0.022274913266301155)\n",
      "(226, 0.021954873576760292)\n",
      "(227, 0.021639276295900345)\n",
      "(228, 0.021328292787075043)\n",
      "(229, 0.02102176658809185)\n",
      "(230, 0.02071964554488659)\n",
      "(231, 0.0204219538718462)\n",
      "(232, 0.020128412172198296)\n",
      "(233, 0.0198390930891037)\n",
      "(234, 0.019554022699594498)\n",
      "(235, 0.01927300915122032)\n",
      "(236, 0.018996048718690872)\n",
      "(237, 0.018723001703619957)\n",
      "(238, 0.01845388300716877)\n",
      "(239, 0.018188713118433952)\n",
      "(240, 0.01792730577290058)\n",
      "(241, 0.01766970008611679)\n",
      "(242, 0.017415747046470642)\n",
      "(243, 0.017165444791316986)\n",
      "(244, 0.016918761655688286)\n",
      "(245, 0.016675597056746483)\n",
      "(246, 0.01643592305481434)\n",
      "(247, 0.016199760138988495)\n",
      "(248, 0.01596694625914097)\n",
      "(249, 0.01573740504682064)\n",
      "(250, 0.015511272475123405)\n",
      "(251, 0.01528835017234087)\n",
      "(252, 0.015068676322698593)\n",
      "(253, 0.014852114953100681)\n",
      "(254, 0.014638634398579597)\n",
      "(255, 0.014428284019231796)\n",
      "(256, 0.014220891520380974)\n",
      "(257, 0.014016508124768734)\n",
      "(258, 0.013815073296427727)\n",
      "(259, 0.013616514392197132)\n",
      "(260, 0.013420861214399338)\n",
      "(261, 0.013227968476712704)\n",
      "(262, 0.01303785853087902)\n",
      "(263, 0.012850443832576275)\n",
      "(264, 0.012665817514061928)\n",
      "(265, 0.012483801692724228)\n",
      "(266, 0.012304386124014854)\n",
      "(267, 0.012127506546676159)\n",
      "(268, 0.011953203938901424)\n",
      "(269, 0.0117814801633358)\n",
      "(270, 0.011612121015787125)\n",
      "(271, 0.011445252224802971)\n",
      "(272, 0.011280776932835579)\n",
      "(273, 0.011118600144982338)\n",
      "(274, 0.010958876460790634)\n",
      "(275, 0.010801341384649277)\n",
      "(276, 0.010646111331880093)\n",
      "(277, 0.010493112727999687)\n",
      "(278, 0.010342331603169441)\n",
      "(279, 0.010193687863647938)\n",
      "(280, 0.010047180578112602)\n",
      "(281, 0.009902804158627987)\n",
      "(282, 0.00976045522838831)\n",
      "(283, 0.009620163589715958)\n",
      "(284, 0.009481917135417461)\n",
      "(285, 0.009345660917460918)\n",
      "(286, 0.009211355820298195)\n",
      "(287, 0.009078959003090858)\n",
      "(288, 0.00894845649600029)\n",
      "(289, 0.00881984829902649)\n",
      "(290, 0.008693119511008263)\n",
      "(291, 0.00856819562613964)\n",
      "(292, 0.008445043116807938)\n",
      "(293, 0.008323666639626026)\n",
      "(294, 0.008204064331948757)\n",
      "(295, 0.0080861896276474)\n",
      "(296, 0.007969983853399754)\n",
      "(297, 0.007855376228690147)\n",
      "(298, 0.007742499001324177)\n",
      "(299, 0.0076312106102705)\n",
      "(300, 0.007521585561335087)\n",
      "(301, 0.007413479033857584)\n",
      "(302, 0.007306915707886219)\n",
      "(303, 0.00720192352309823)\n",
      "(304, 0.0070983911864459515)\n",
      "(305, 0.006996410898864269)\n",
      "(306, 0.006895849481225014)\n",
      "(307, 0.0067967684008181095)\n",
      "(308, 0.006699071265757084)\n",
      "(309, 0.006602793466299772)\n",
      "(310, 0.006507884245365858)\n",
      "(311, 0.006414397619664669)\n",
      "(312, 0.006322198081761599)\n",
      "(313, 0.006231348961591721)\n",
      "(314, 0.006141782272607088)\n",
      "(315, 0.00605352409183979)\n",
      "(316, 0.0059664929285645485)\n",
      "(317, 0.005880739074200392)\n",
      "(318, 0.005796252284198999)\n",
      "(319, 0.005712925456464291)\n",
      "(320, 0.005630865693092346)\n",
      "(321, 0.005549905821681023)\n",
      "(322, 0.005470138508826494)\n",
      "(323, 0.005391564220190048)\n",
      "(324, 0.00531406607478857)\n",
      "(325, 0.005237693898379803)\n",
      "(326, 0.005162395536899567)\n",
      "(327, 0.005088239908218384)\n",
      "(328, 0.005015119444578886)\n",
      "(329, 0.004943010397255421)\n",
      "(330, 0.004871967248618603)\n",
      "(331, 0.004801945760846138)\n",
      "(332, 0.004732975270599127)\n",
      "(333, 0.004664924927055836)\n",
      "(334, 0.0045978836715221405)\n",
      "(335, 0.004531813319772482)\n",
      "(336, 0.004466698504984379)\n",
      "(337, 0.004402494989335537)\n",
      "(338, 0.0043392120860517025)\n",
      "(339, 0.0042768665589392185)\n",
      "(340, 0.004215371794998646)\n",
      "(341, 0.004154798574745655)\n",
      "(342, 0.0040951138362288475)\n",
      "(343, 0.00403624540194869)\n",
      "(344, 0.003978228662163019)\n",
      "(345, 0.003921083174645901)\n",
      "(346, 0.0038646908942610025)\n",
      "(347, 0.003809169866144657)\n",
      "(348, 0.0037544211372733116)\n",
      "(349, 0.003700464963912964)\n",
      "(350, 0.0036472610663622618)\n",
      "(351, 0.003594867419451475)\n",
      "(352, 0.0035431887954473495)\n",
      "(353, 0.003492265474051237)\n",
      "(354, 0.0034420930314809084)\n",
      "(355, 0.0033926120959222317)\n",
      "(356, 0.0033438459504395723)\n",
      "(357, 0.0032958064693957567)\n",
      "(358, 0.0032484359107911587)\n",
      "(359, 0.0032017508056014776)\n",
      "(360, 0.0031557430047541857)\n",
      "(361, 0.003110382705926895)\n",
      "(362, 0.0030656694434583187)\n",
      "(363, 0.0030216160230338573)\n",
      "(364, 0.002978201489895582)\n",
      "(365, 0.0029353941790759563)\n",
      "(366, 0.0028931968845427036)\n",
      "(367, 0.00285163102671504)\n",
      "(368, 0.002810665173456073)\n",
      "(369, 0.002770267892628908)\n",
      "(370, 0.002730436623096466)\n",
      "(371, 0.0026912072207778692)\n",
      "(372, 0.0026525314897298813)\n",
      "(373, 0.0026144091971218586)\n",
      "(374, 0.0025768191553652287)\n",
      "(375, 0.0025397860445082188)\n",
      "(376, 0.002503282856196165)\n",
      "(377, 0.0024673091247677803)\n",
      "(378, 0.002431865781545639)\n",
      "(379, 0.002396900672465563)\n",
      "(380, 0.002362476894631982)\n",
      "(381, 0.0023285099305212498)\n",
      "(382, 0.002295047976076603)\n",
      "(383, 0.0022620493546128273)\n",
      "(384, 0.0022295559756457806)\n",
      "(385, 0.002197521273046732)\n",
      "(386, 0.002165929414331913)\n",
      "(387, 0.0021347999572753906)\n",
      "(388, 0.0021041277796030045)\n",
      "(389, 0.0020738630555570126)\n",
      "(390, 0.002044075168669224)\n",
      "(391, 0.0020146984606981277)\n",
      "(392, 0.001985739218071103)\n",
      "(393, 0.0019572079181671143)\n",
      "(394, 0.0019290775526314974)\n",
      "(395, 0.0019013509154319763)\n",
      "(396, 0.0018740136874839664)\n",
      "(397, 0.0018471019575372338)\n",
      "(398, 0.001820550998672843)\n",
      "(399, 0.0017943967832252383)\n",
      "(400, 0.001768607646226883)\n",
      "(401, 0.001743187429383397)\n",
      "(402, 0.0017181222792714834)\n",
      "(403, 0.0016934468876570463)\n",
      "(404, 0.0016691114287823439)\n",
      "(405, 0.0016451101982966065)\n",
      "(406, 0.001621453557163477)\n",
      "(407, 0.0015981659526005387)\n",
      "(408, 0.0015751805622130632)\n",
      "(409, 0.0015525518683716655)\n",
      "(410, 0.0015302574029192328)\n",
      "(411, 0.0015082370955497026)\n",
      "(412, 0.0014865691773593426)\n",
      "(413, 0.0014652026584371924)\n",
      "(414, 0.0014441567473113537)\n",
      "(415, 0.0014234120026230812)\n",
      "(416, 0.0014029437443241477)\n",
      "(417, 0.0013827686198055744)\n",
      "(418, 0.001362919807434082)\n",
      "(419, 0.0013433104613795877)\n",
      "(420, 0.0013240125263109803)\n",
      "(421, 0.0013049885164946318)\n",
      "(422, 0.0012862315634265542)\n",
      "(423, 0.0012677572667598724)\n",
      "(424, 0.0012495401315391064)\n",
      "(425, 0.0012315729400143027)\n",
      "(426, 0.0012138696620240808)\n",
      "(427, 0.0011964152799919248)\n",
      "(428, 0.0011792388977482915)\n",
      "(429, 0.0011622919701039791)\n",
      "(430, 0.001145585672929883)\n",
      "(431, 0.001129115466028452)\n",
      "(432, 0.0011128946207463741)\n",
      "(433, 0.001096904743462801)\n",
      "(434, 0.0010811357060447335)\n",
      "(435, 0.001065606134943664)\n",
      "(436, 0.0010502830846235156)\n",
      "(437, 0.0010351907694712281)\n",
      "(438, 0.0010203098645433784)\n",
      "(439, 0.0010056397877633572)\n",
      "(440, 0.0009911968372762203)\n",
      "(441, 0.000976949231699109)\n",
      "(442, 0.0009629202540963888)\n",
      "(443, 0.0009490823722444475)\n",
      "(444, 0.0009354263893328607)\n",
      "(445, 0.0009219858329743147)\n",
      "(446, 0.0009087354992516339)\n",
      "(447, 0.0008956872043199837)\n",
      "(448, 0.0008828075369819999)\n",
      "(449, 0.0008701086044311523)\n",
      "(450, 0.0008576225955039263)\n",
      "(451, 0.000845299509819597)\n",
      "(452, 0.0008331394637934864)\n",
      "(453, 0.0008211674867197871)\n",
      "(454, 0.0008093664655461907)\n",
      "(455, 0.0007977263885550201)\n",
      "(456, 0.0007862701895646751)\n",
      "(457, 0.0007749712094664574)\n",
      "(458, 0.0007638311362825334)\n",
      "(459, 0.0007528626592829823)\n",
      "(460, 0.0007420289912261069)\n",
      "(461, 0.0007313775713555515)\n",
      "(462, 0.0007208515889942646)\n",
      "(463, 0.0007105087861418724)\n",
      "(464, 0.000700294622220099)\n",
      "(465, 0.0006902246968820691)\n",
      "(466, 0.0006802943535149097)\n",
      "(467, 0.0006705278065055609)\n",
      "(468, 0.0006608976982533932)\n",
      "(469, 0.0006513936095871031)\n",
      "(470, 0.0006420231657102704)\n",
      "(471, 0.0006327981827780604)\n",
      "(472, 0.0006237038178369403)\n",
      "(473, 0.0006147415842860937)\n",
      "(474, 0.0006059171864762902)\n",
      "(475, 0.0005972106591798365)\n",
      "(476, 0.0005886192666366696)\n",
      "(477, 0.0005801643710583448)\n",
      "(478, 0.0005718220490962267)\n",
      "(479, 0.000563613313715905)\n",
      "(480, 0.0005555133102461696)\n",
      "(481, 0.0005475249490700662)\n",
      "(482, 0.0005396633059717715)\n",
      "(483, 0.0005319001502357423)\n",
      "(484, 0.0005242610350251198)\n",
      "(485, 0.0005167288472875953)\n",
      "(486, 0.0005092851934023201)\n",
      "(487, 0.0005019698292016983)\n",
      "(488, 0.0004947630222886801)\n",
      "(489, 0.00048764547682367265)\n",
      "(490, 0.0004806483630090952)\n",
      "(491, 0.0004737271519843489)\n",
      "(492, 0.00046692523756064475)\n",
      "(493, 0.0004602174158208072)\n",
      "(494, 0.0004535940242931247)\n",
      "(495, 0.0004470752028282732)\n",
      "(496, 0.0004406596999615431)\n",
      "(497, 0.0004343257169239223)\n",
      "(498, 0.0004280820721760392)\n",
      "(499, 0.0004219349939376116)\n",
      "('predict (after training)', 4, 7.9763875007629395)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out # ( 인풋사이즈=1, 아웃풋사이즈=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred    \n",
    "    \n",
    "    \n",
    "# our model\n",
    "model = Model()        \n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    # criterion \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # 이게 웨이트 업데이트 하는거?\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\",  4, model(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predict', 7.972508430480957)\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred= self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred,y_data)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "#트레이닝 후에 4를 넣어보기\n",
    "\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "\n",
    "y_pred = model(hour_var)\n",
    "print (\"predict\",model(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
